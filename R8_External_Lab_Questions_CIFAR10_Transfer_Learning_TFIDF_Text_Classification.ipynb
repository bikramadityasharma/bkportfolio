{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QGIsF1ADyJ58"
   },
   "source": [
    "# Transfer Learning CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-n6tVFayGBe"
   },
   "source": [
    "* Train a simple convnet on the CIFAR dataset the first 5 output classes [0..4].\n",
    "* Freeze convolutional layers and fine-tune dense layers for the last 5 ouput classes [5..9].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cq8ejXHJyGYq"
   },
   "source": [
    "### 1. Import CIFAR10 data and create 2 datasets with one dataset having classes from 0 to 4 and other having classes from 5 to 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWYbxnBayFUP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data sample : [[[251 249 250]\n",
      "  [247 245 247]\n",
      "  [247 245 247]\n",
      "  ...\n",
      "  [229 190 146]\n",
      "  [244 231 224]\n",
      "  [251 241 241]]\n",
      "\n",
      " [[249 248 248]\n",
      "  [246 244 244]\n",
      "  [246 245 244]\n",
      "  ...\n",
      "  [233 188 141]\n",
      "  [249 237 233]\n",
      "  [252 242 241]]\n",
      "\n",
      " [[167 165 148]\n",
      "  [167 164 148]\n",
      "  [167 164 149]\n",
      "  ...\n",
      "  [217 182 139]\n",
      "  [217 211 203]\n",
      "  [220 213 208]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[133 130  39]\n",
      "  [123 127  36]\n",
      "  [124 130  35]\n",
      "  ...\n",
      "  [118 125  30]\n",
      "  [114 122  26]\n",
      "  [115 125  27]]\n",
      "\n",
      " [[123 125  36]\n",
      "  [124 127  36]\n",
      "  [126 129  32]\n",
      "  ...\n",
      "  [112 122  26]\n",
      "  [108 119  27]\n",
      "  [104 119  22]]\n",
      "\n",
      " [[125 128  42]\n",
      "  [129 132  43]\n",
      "  [126 130  36]\n",
      "  ...\n",
      "  [118 128  35]\n",
      "  [112 122  33]\n",
      "  [105 121  26]]]\n",
      "test data sample : [[[ 66  73  84]\n",
      "  [ 66  71  81]\n",
      "  [ 64  67  75]\n",
      "  ...\n",
      "  [104  97  81]\n",
      "  [104  97  81]\n",
      "  [104  97  81]]\n",
      "\n",
      " [[ 51  57  67]\n",
      "  [ 55  59  68]\n",
      "  [ 49  51  58]\n",
      "  ...\n",
      "  [ 94  87  71]\n",
      "  [ 95  88  72]\n",
      "  [ 96  89  73]]\n",
      "\n",
      " [[ 49  53  63]\n",
      "  [ 55  58  67]\n",
      "  [ 48  49  55]\n",
      "  ...\n",
      "  [ 78  69  55]\n",
      "  [ 77  68  54]\n",
      "  [ 78  69  56]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[153 156 133]\n",
      "  [128 129 107]\n",
      "  [ 80  79  63]\n",
      "  ...\n",
      "  [184 184 140]\n",
      "  [183 183 139]\n",
      "  [182 182 138]]\n",
      "\n",
      " [[160 164 136]\n",
      "  [142 145 117]\n",
      "  [100  99  79]\n",
      "  ...\n",
      "  [184 189 143]\n",
      "  [184 188 143]\n",
      "  [183 187 142]]\n",
      "\n",
      " [[165 171 139]\n",
      "  [153 156 123]\n",
      "  [120 120  97]\n",
      "  ...\n",
      "  [184 193 146]\n",
      "  [184 192 145]\n",
      "  [183 191 144]]]\n",
      "train label sample : [0]\n",
      "test label sample : [2]\n"
     ]
    }
   ],
   "source": [
    "# printing  train & test data samples\n",
    "print(\"train data sample :\", X_train[35])\n",
    "print(\"test data sample :\", X_test[35])\n",
    "print(\"train label sample :\", y_train[35])\n",
    "print(\"test label sample :\", y_test[35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: [4]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d7e7ff9588>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualising data\n",
    "print(\"Label: {}\".format(y_train[20]))\n",
    "plt.imshow(X_train[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n",
      "[0]\n",
      "[7]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Splitting data set into 2 categories one with 0-4 labels & other with 5-9 labels.\n",
    "\n",
    "x_train_zero_four = []\n",
    "y_train_zero_four = []\n",
    "x_train_five_nine = []\n",
    "y_train_five_nine = []\n",
    "\n",
    "x_test_zero_four = []\n",
    "y_test_zero_four = []\n",
    "x_test_five_nine = []\n",
    "y_test_five_nine = []\n",
    "\n",
    "for idx,x in enumerate(y_train):\n",
    "    if (x>4):\n",
    "        x_train_five_nine.append(X_train[idx])\n",
    "        y_train_five_nine.append(y_train[idx])\n",
    "    else:\n",
    "        x_train_zero_four.append(X_train[idx])\n",
    "        y_train_zero_four.append(y_train[idx])\n",
    "\n",
    "        \n",
    "for idx,y in enumerate(y_test):\n",
    "    if (y>4):\n",
    "        x_test_five_nine.append(X_test[idx])\n",
    "        y_test_five_nine.append(y_test[idx])\n",
    "    else:\n",
    "        x_test_zero_four.append(X_test[idx])\n",
    "        y_test_zero_four.append(y_test[idx])\n",
    "        \n",
    "print(y_train_five_nine[14])\n",
    "print(y_train_zero_four[14]) \n",
    "\n",
    "print(y_test_five_nine[14])\n",
    "print(y_test_zero_four[14]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 32, 32, 3)\n",
      "(25000, 1)\n",
      "(25000, 32, 32, 3)\n",
      "(25000, 1)\n",
      "(5000, 32, 32, 3)\n",
      "(5000, 1)\n",
      "(5000, 32, 32, 3)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "# printing shape of each data set post splitting\n",
    "\n",
    "x_train_zero_four = np.asarray(x_train_zero_four,dtype='float')\n",
    "y_train_zero_four = np.asarray(y_train_zero_four,dtype='float')\n",
    "x_train_five_nine = np.asarray(x_train_five_nine,dtype='float')\n",
    "y_train_five_nine = np.asarray(y_train_five_nine,dtype='float')\n",
    "x_test_zero_four = np.asarray(x_test_zero_four,dtype='float')\n",
    "y_test_zero_four = np.asarray(y_test_zero_four,dtype='float')\n",
    "x_test_five_nine = np.asarray(x_test_five_nine,dtype='float')\n",
    "y_test_five_nine = np.asarray(y_test_five_nine,dtype='float')\n",
    "print(x_train_zero_four.shape)\n",
    "print(y_train_zero_four.shape)\n",
    "print(x_train_five_nine.shape)\n",
    "print(y_train_five_nine.shape)\n",
    "print(x_test_zero_four.shape)\n",
    "print(y_test_zero_four.shape)\n",
    "print(x_test_five_nine.shape)\n",
    "print(y_test_five_nine.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping data\n",
    "\n",
    "x_train_zero_four = x_train_zero_four.reshape(x_train_zero_four.shape[0], 32, 32, 3).astype('float32')\n",
    "x_test_zero_four = x_test_zero_four.reshape(x_test_zero_four.shape[0], 32, 32, 3).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise data\n",
    "\n",
    "x_train_zero_four /= 255\n",
    "x_test_zero_four /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 32, 32, 3)\n",
      "(5000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_zero_four.shape)\n",
    "print(y_test_zero_four.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xtCKmQh4yXhT"
   },
   "source": [
    "### 2. Use One-hot encoding to divide y_train and y_test into required no of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uN5O2kJ3yYa6"
   },
   "outputs": [],
   "source": [
    "y_train_zero_four = np_utils.to_categorical(y_train_zero_four, 5)\n",
    "y_test_zero_four = np_utils.to_categorical(y_test_zero_four, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cuOiKWfeybAl"
   },
   "source": [
    "### 3. Build a sequential neural network model which can classify the classes 0 to 4 of CIFAR10 dataset with at least 80% accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5HzxNbiiyoBD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 15s 607us/sample - loss: 1.1467 - accuracy: 0.5277 - val_loss: 0.9362 - val_accuracy: 0.6268\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 12s 479us/sample - loss: 0.9464 - accuracy: 0.6282 - val_loss: 0.8385 - val_accuracy: 0.6622\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 12s 473us/sample - loss: 0.8810 - accuracy: 0.6566 - val_loss: 0.8046 - val_accuracy: 0.6928\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 12s 488us/sample - loss: 0.8331 - accuracy: 0.6793 - val_loss: 0.7743 - val_accuracy: 0.6924\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 12s 499us/sample - loss: 0.7928 - accuracy: 0.6964 - val_loss: 0.7555 - val_accuracy: 0.7072\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 12s 488us/sample - loss: 0.7548 - accuracy: 0.7101 - val_loss: 0.7798 - val_accuracy: 0.7008\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 19s 761us/sample - loss: 0.7163 - accuracy: 0.7270 - val_loss: 0.7229 - val_accuracy: 0.7142\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 12s 496us/sample - loss: 0.6798 - accuracy: 0.7395 - val_loss: 0.7359 - val_accuracy: 0.7084\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 13s 501us/sample - loss: 0.6492 - accuracy: 0.7515 - val_loss: 0.7210 - val_accuracy: 0.7160\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 13s 519us/sample - loss: 0.6170 - accuracy: 0.7646 - val_loss: 0.7462 - val_accuracy: 0.7086\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 13s 517us/sample - loss: 0.6004 - accuracy: 0.7719 - val_loss: 0.6974 - val_accuracy: 0.7306\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 14s 544us/sample - loss: 0.5621 - accuracy: 0.7901 - val_loss: 0.6908 - val_accuracy: 0.7382\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 12s 499us/sample - loss: 0.5411 - accuracy: 0.7956 - val_loss: 0.6961 - val_accuracy: 0.7388\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 12s 484us/sample - loss: 0.5185 - accuracy: 0.8045 - val_loss: 0.7142 - val_accuracy: 0.7346\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 12s 491us/sample - loss: 0.4912 - accuracy: 0.8166 - val_loss: 0.7431 - val_accuracy: 0.7272\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 12s 498us/sample - loss: 0.4720 - accuracy: 0.8221 - val_loss: 0.7363 - val_accuracy: 0.7374\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 13s 509us/sample - loss: 0.4575 - accuracy: 0.8276 - val_loss: 0.7376 - val_accuracy: 0.7326\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 13s 518us/sample - loss: 0.4380 - accuracy: 0.8378 - val_loss: 0.7227 - val_accuracy: 0.7402\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 13s 520us/sample - loss: 0.4241 - accuracy: 0.8424 - val_loss: 0.7543 - val_accuracy: 0.7328\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 13s 540us/sample - loss: 0.4117 - accuracy: 0.8457 - val_loss: 0.7445 - val_accuracy: 0.7338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d7f480de10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    " # 1st Conv Layer\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Max Pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    " # Dropout\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fully Connected Layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#Dense\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# More Dropout\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Loss and Optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_zero_four, y_train_zero_four, batch_size=32, epochs=20,validation_data=(x_test_zero_four, y_test_zero_four))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "woTfNst_ynRG"
   },
   "source": [
    "### 4. In the model which was built above (for classification of classes 0-4 in CIFAR10), make only the dense layers to be trainable and conv layers to be non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_VCDB3Byb1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mconv2d\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mactivation\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mmax_pooling2d\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdropout\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mflatten\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdense\u001b[0m\n",
      "\u001b[31mTrue\u001b[0m\n",
      "\u001b[34mactivation_1\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdropout_1\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdense_1\u001b[0m\n",
      "\u001b[31mTrue\u001b[0m\n",
      "\u001b[34mactivation_2\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdropout_2\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n",
      "\u001b[34mdense_2\u001b[0m\n",
      "\u001b[31mTrue\u001b[0m\n",
      "\u001b[34mactivation_3\u001b[0m\n",
      "\u001b[31mFalse\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Freezing layers in the model which don't have 'dense' in their name\n",
    "for layer in model.layers:\n",
    "  if('dense' not in layer.name): #prefix detection to freeze layers which does not have dense\n",
    "    #Freezing a layer\n",
    "    layer.trainable = False\n",
    "\n",
    "#Module to print colourful statements\n",
    "from termcolor import colored\n",
    "\n",
    "#Check which layers have been frozen \n",
    "for layer in model.layers:\n",
    "  print (colored(layer.name, 'blue'))\n",
    "  print (colored(layer.trainable, 'red'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-uUPqWpyeyX"
   },
   "source": [
    "### 5. Utilize the the model trained on CIFAR 10 (classes 0 to 4) to classify the classes 5 to 9 of CIFAR 10  (Use Transfer Learning) <br>\n",
    "Achieve an accuracy of more than 85% on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape\n",
    "x_train_five_nine = x_train_five_nine.reshape(x_train_five_nine.shape[0], 32, 32, 3).astype('float32')\n",
    "x_test_five_nine = x_test_five_nine.reshape(x_test_five_nine.shape[0], 32, 32, 3).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalise\n",
    "x_train_five_nine /= 255\n",
    "x_test_five_nine /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "szHjJgDvyfCt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.]\n"
     ]
    }
   ],
   "source": [
    "# encoding the labels for 5-9 of cifar 10 data set\n",
    "y_train_five_nine = y_train_five_nine -5\n",
    "y_test_five_nine = y_test_five_nine - 5\n",
    "\n",
    "print(y_test_five_nine[0])\n",
    "\n",
    "y_train_five_nine = np_utils.to_categorical(y_train_five_nine, 5)\n",
    "y_test_five_nine = np_utils.to_categorical(y_test_five_nine, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 12s 492us/sample - loss: 1.1143 - accuracy: 0.5708 - val_loss: 0.7653 - val_accuracy: 0.7228\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 12s 483us/sample - loss: 0.8237 - accuracy: 0.6911 - val_loss: 0.6673 - val_accuracy: 0.7540\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 12s 491us/sample - loss: 0.7484 - accuracy: 0.7242 - val_loss: 0.6195 - val_accuracy: 0.7698\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 13s 512us/sample - loss: 0.6867 - accuracy: 0.7507 - val_loss: 0.6017 - val_accuracy: 0.7784\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 13s 508us/sample - loss: 0.6444 - accuracy: 0.7638 - val_loss: 0.5550 - val_accuracy: 0.8000\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 13s 509us/sample - loss: 0.6086 - accuracy: 0.7792 - val_loss: 0.5609 - val_accuracy: 0.7882\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 13s 518us/sample - loss: 0.5716 - accuracy: 0.7892 - val_loss: 0.5221 - val_accuracy: 0.8066\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 13s 512us/sample - loss: 0.5400 - accuracy: 0.8056 - val_loss: 0.5035 - val_accuracy: 0.8110\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 12s 461us/sample - loss: 0.5165 - accuracy: 0.8103 - val_loss: 0.4928 - val_accuracy: 0.8180\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 12s 461us/sample - loss: 0.4876 - accuracy: 0.8245 - val_loss: 0.5021 - val_accuracy: 0.8158\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 12s 472us/sample - loss: 0.4612 - accuracy: 0.8312 - val_loss: 0.4795 - val_accuracy: 0.8216\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 12s 473us/sample - loss: 0.4384 - accuracy: 0.8388 - val_loss: 0.4758 - val_accuracy: 0.8292\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 12s 476us/sample - loss: 0.4135 - accuracy: 0.8494 - val_loss: 0.4757 - val_accuracy: 0.8280\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 15s 594us/sample - loss: 0.3995 - accuracy: 0.8553 - val_loss: 0.5097 - val_accuracy: 0.8168\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 14s 578us/sample - loss: 0.3757 - accuracy: 0.8611 - val_loss: 0.4907 - val_accuracy: 0.8268\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 15s 594us/sample - loss: 0.3655 - accuracy: 0.8677 - val_loss: 0.4877 - val_accuracy: 0.8290\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 15s 607us/sample - loss: 0.3398 - accuracy: 0.8754 - val_loss: 0.5030 - val_accuracy: 0.8228\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 15s 618us/sample - loss: 0.3223 - accuracy: 0.8827 - val_loss: 0.4817 - val_accuracy: 0.8332\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 16s 645us/sample - loss: 0.3116 - accuracy: 0.8888 - val_loss: 0.5369 - val_accuracy: 0.8220\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 16s 652us/sample - loss: 0.3044 - accuracy: 0.8905 - val_loss: 0.4887 - val_accuracy: 0.8308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d7f653c748>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transfer learning . Fit the model built on 0-4 data onto data 5-9\n",
    "model.fit(x_train_five_nine, y_train_five_nine, batch_size=32, epochs=20,validation_data=(x_test_five_nine, y_test_five_nine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0zDuRecXzEtr"
   },
   "source": [
    "# Text classification using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xMPlEJhHzb6P"
   },
   "source": [
    "### 6. Load the dataset from sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fe-B59u3zHNb"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRrMemVQzbHU"
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-sZX0UbJzmg5"
   },
   "source": [
    "### 7. Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CITr_5aXziJ2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['From: sd345@city.ac.uk (Michael Collier)\\nSubject: Converting images to HP LaserJet III?\\nNntp-Posting-Host: hampton\\nOrganization: The City University\\nLines: 14\\n\\nDoes anyone know of a good way (standard PC application/PD utility) to\\nconvert tif/img/tga files into LaserJet III format.  We would also like to\\ndo the same, converting to HPGL (HP plotter) files.\\n\\nPlease email any response.\\n\\nIs this the correct group?\\n\\nThanks in advance.  Michael.\\n-- \\nMichael Collier (Programmer)                 The Computer Unit,\\nEmail: M.P.Collier@uk.ac.city                The City University,\\nTel: 071 477-8000 x3769                      London,\\nFax: 071 477-8565                            EC1V 0HB.\\n',\n",
       " \"From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\nSubject: help: Splitting a trimming region along a mesh \\nOrganization: University Of Kentucky, Dept. of Math Sciences\\nLines: 28\\n\\n\\n\\n\\tHi,\\n\\n\\tI have a problem, I hope some of the 'gurus' can help me solve.\\n\\n\\tBackground of the problem:\\n\\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \\n\\tmapping of a 3d Bezier patch into 2d. The area in this domain\\n\\twhich is inside a trimming loop had to be rendered. The trimming\\n\\tloop is a set of 2d Bezier curve segments.\\n\\tFor the sake of notation: the mesh is made up of cells.\\n\\n\\tMy problem is this :\\n\\tThe trimming area has to be split up into individual smaller\\n\\tcells bounded by the trimming curve segments. If a cell\\n\\tis wholly inside the area...then it is output as a whole ,\\n\\telse it is trivially rejected. \\n\\n\\tDoes any body know how thiss can be done, or is there any algo. \\n\\tsomewhere for doing this.\\n\\n\\tAny help would be appreciated.\\n\\n\\tThanks, \\n\\tAni.\\n-- \\nTo get irritated is human, to stay cool, divine.\\n\",\n",
       " \"From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSubject: Re: harrassed at work, could use some prayers\\nOrganization: =CSE Dept., U.C. San Diego\\nLines: 63\\n\\n(Well, I'll email also, but this may apply to other people, so\\nI'll post also.)\\n\\n>I've been working at this company for eight years in various\\n>engineering jobs.  I'm female.  Yesterday I counted and realized that\\n>on seven different occasions I've been sexually harrassed at this\\n>company.\\n\\n>I dreaded coming back to work today.  What if my boss comes in to ask\\n>me some kind of question...\\n\\nYour boss should be the person bring these problems to.  If he/she\\ndoes not seem to take any action, keep going up higher and higher.\\nSexual harrassment does not need to be tolerated, and it can be an\\nenormous emotional support to discuss this with someone and know that\\nthey are trying to do something about it.  If you feel you can not\\ndiscuss this with your boss, perhaps your company has a personnel\\ndepartment that can work for you while preserving your privacy.  Most\\ncompanies will want to deal with this problem because constant anxiety\\ndoes seriously affect how effectively employees do their jobs.\\n\\nIt is unclear from your letter if you have done this or not.  It is\\nnot inconceivable that management remains ignorant of employee\\nproblems/strife even after eight years (it's a miracle if they do\\nnotice).  Perhaps your manager did not bring to the attention of\\nhigher ups?  If the company indeed does seem to want to ignore the\\nentire problem, there may be a state agency willing to fight with\\nyou.  (check with a lawyer, a women's resource center, etc to find out)\\n\\nYou may also want to discuss this with your paster, priest, husband,\\netc.  That is, someone you know will not be judgemental and that is\\nsupportive, comforting, etc.  This will bring a lot of healing.\\n\\n>So I returned at 11:25, only to find that ever single\\n>person had already left for lunch.  They left at 11:15 or so.  No one\\n>could be bothered to call me at the other building, even though my\\n>number was posted.\\n\\nThis happens to a lot of people.  Honest.  I believe it may seem\\nto be due to gross insensitivity because of the feelings you are\\ngoing through.  People in offices tend to be more insensitive while\\nworking than they normally are (maybe it's the hustle or stress or...)\\nI've had this happen to me a lot, often because they didn't realize\\nmy car was broken, etc.  Then they will come back and wonder why I\\ndidn't want to go (this would tend to make me stop being angry at\\nbeing ignored and make me laugh).  Once, we went off without our\\nboss, who was paying for the lunch :-)\\n\\n>For this\\n>reason I hope good Mr. Moderator allows me this latest indulgence.\\n\\nWell, if you can't turn to the computer for support, what would\\nwe do?  (signs of the computer age :-)\\n\\nIn closing, please don't let the hateful actions of a single person\\nharm you.  They are doing it because they are still the playground\\nbully and enjoy seeing the hurt they cause.  And you should not\\naccept the opinions of an imbecile that you are worthless - much\\nwiser people hold you in great esteem.\\n-- \\nDarin Johnson\\ndjohnson@ucsd.edu\\n  - Luxury!  In MY day, we had to make do with 5 bytes of swap...\\n\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "twenty_train.data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xcESc5QXzr6p"
   },
   "source": [
    "### 8. Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysInblUMzpvl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"From: brian@ucsd.edu (Brian Kantor)\\nSubject: Re: HELP for Kidney Stones ..............\\nOrganization: The Avant-Garde of the Now, Ltd.\\nLines: 12\\nNNTP-Posting-Host: ucsd.edu\\n\\nAs I recall from my bout with kidney stones, there isn't any\\nmedication that can do anything about them except relieve the pain.\\n\\nEither they pass, or they have to be broken up with sound, or they have\\nto be extracted surgically.\\n\\nWhen I was in, the X-ray tech happened to mention that she'd had kidney\\nstones and children, and the childbirth hurt less.\\n\\nDemerol worked, although I nearly got arrested on my way home when I barfed\\nall over the police car parked just outside the ER.\\n\\t- Brian\\n\",\n",
       " 'From: rind@enterprise.bih.harvard.edu (David Rind)\\nSubject: Re: Candida(yeast) Bloom, Fact or Fiction\\nOrganization: Beth Israel Hospital, Harvard Medical School, Boston Mass., USA\\nLines: 37\\nNNTP-Posting-Host: enterprise.bih.harvard.edu\\n\\nIn article <1993Apr26.103242.1@vms.ocom.okstate.edu>\\n banschbach@vms.ocom.okstate.edu writes:\\n>are in a different class.  The big question seems to be is it reasonable to \\n>use them in patients with GI distress or sinus problems that *could* be due \\n>to candida blooms following the use of broad-spectrum antibiotics?\\n\\nI guess I\\'m still not clear on what the term \"candida bloom\" means,\\nbut certainly it is well known that thrush (superficial candidal\\ninfections on mucous membranes) can occur after antibiotic use.\\nThis has nothing to do with systemic yeast syndrome, the \"quack\"\\ndiagnosis that has been being discussed.\\n\\n\\n>found in the sinus mucus membranes than is candida.  Women have been known \\n>for a very long time to suffer from candida blooms in the vagina and a \\n>women is lucky to find a physician who is willing to treat the cause and \\n>not give give her advise to use the OTC anti-fungal creams.\\n\\nLucky how?  Since a recent article (randomized controlled trial) of\\noral yogurt on reducing vaginal candidiasis, I\\'ve mentioned to a \\nnumber of patients with frequent vaginal yeast infections that they\\ncould try eating 6 ounces of yogurt daily.  It turns out most would\\nrather just use anti-fungal creams when they get yeast infections.\\n\\n>yogurt dangerous).  If this were a standard part of medical practice, as \\n>Gordon R. says it is, then the incidence of GI distress and vaginal yeast \\n>infections should decline.\\n\\nAgain, this just isn\\'t what the systemic yeast syndrome is about, and\\nhas nothing to do with the quack therapies that were being discussed.\\nThere is some evidence that attempts to reinoculate the GI tract with\\nbacteria after antibiotic therapy don\\'t seem to be very helpful in\\nreducing diarrhea, but I don\\'t think anyone would view this as a\\nquack therapy.\\n-- \\nDavid Rind\\nrind@enterprise.bih.harvard.edu\\n',\n",
       " 'From: adwright@iastate.edu ()\\nSubject: Re: centi- and milli- pedes\\nOrganization: Iowa State University, Ames IA\\nLines: 37\\n\\nIn <1993Apr29.112642.1@vms.ocom.okstate.edu> chorley@vms.ocom.okstate.edu writes:\\n\\n>In article <35004@castle.ed.ac.uk>, gtclark@festival.ed.ac.uk (G T Clark) writes:\\n>> msnyder@nmt.edu (Rebecca Snyder) writes:\\n>> \\n>>>Does anyone know how posionous centipedes and millipedes are? If someone\\n>>>was bitten, how soon would medical treatment be needed, and what would\\n>>>be liable to happen to the person?\\n>> \\n>>>(Just for clarification - I have NOT been bitten by one of these,  but my\\n>>>house seems to be infested, and I want to know \\'just in case\\'.)\\n>> \\n>>>Rebecca\\n>> \\n>> \\n>> \\tMillipedes, I understand, are vegetarian, and therefore almost\\n>> certainly will not bite and are not poisonous. Centipedes are\\n>> carnivorous, and although I don\\'t have any absolute knowledge on this, I\\n>> would tend to think that you\\'re in no danger from anything but a\\n>> concerted assault by several million of them.\\n>> \\n>> \\t\\t\\tG.\\n>Not sure of this but I think some millipedes cause a toxic reaction (sting?\\n>So I would not assume that they are not dangerous merely on the basis of \\n>vegetarianism, after all wasps are vegetarian too.\\n>dnc.\\n\\nAs a child i can remember picking up a centipede and getting a rather painful \\nsting, but it quickly subsided. Much less painful compared to a bee sting. \\nCentipedes have a poison claw (one of the front feet) to stun their prey, but\\nin my single experience it did not have a lot of \"bite\" to it.\\n\\nA.\\n\\n\\n\\n\\n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "twenty_test.data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DriL2yZ50DQq"
   },
   "source": [
    "###  a.  You can access the values for the target variable using .target attribute \n",
    "###  b. You can access the name of the class in the target variable with .target_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vlUuai99z1hX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train set labels (2257,)\n",
      "shape of train set features 2257\n"
     ]
    }
   ],
   "source": [
    "print(\"shape of train set labels\",twenty_train.target.shape)\n",
    "print(\"shape of train set features\",len(twenty_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEKzaDfSz5E-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hTz4EaN_1WGc"
   },
   "source": [
    "### 9.  Now with dependent and independent data available for both train and test datasets, using TfidfVectorizer fit and transform the training data and test data and get the tfidf features for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H5G477f81C0Z"
   },
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer(min_df = 4,ngram_range = (1,2),stop_words = \"english\")\n",
    "X_train = vectorizer.fit_transform(twenty_train.data)\n",
    "X_test = vectorizer.transform(twenty_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tp_fDINJ1t4L"
   },
   "source": [
    "### 10. Use logisticRegression with tfidf features as input and targets as output and train the model and report the train and test accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THlN2b5d1yQp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training score 0.9920248116969429\n",
      "test score 0.9027962716378163\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LRmodel = LogisticRegression()\n",
    "LRmodel.fit(X_train,twenty_train.target)\n",
    "LR_score_train = LRmodel.score(X_train,twenty_train.target)\n",
    "print(\"training score\",LR_score_train)\n",
    "LR_score_test = LRmodel.score(X_test,twenty_test.target)\n",
    "print(\"test score\",LR_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "R8_External_Lab_Questions_CIFAR10_Transfer_Learning_TFIDF_Text_Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
